{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Self supervised learning\n",
    "* Author: Jing Zhang\n",
    "* Date: 2024/10/07\n",
    "* reference:https://docs.lightly.ai/self-supervised-learning/index.html\n",
    "![overview](../figures/lightly_overview.png)    \n",
    "* installation:    \n",
    "`pip install lightly`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import lightly.data as data\n",
    "\n",
    "from lightly.transforms.simclr_transform import SimCLRTransform # augmentation methods for specific models, e.g. SimCLR\n",
    "\n",
    "# The following transform will return two augmented images per input image.\n",
    "transform = SimCLRTransform()\n",
    "\n",
    "# Create a dataset from your image folder.\n",
    "dataset = data.LightlyDataset(\n",
    "    input_dir='path/unlabeled_data/',\n",
    "    transform=transform,\n",
    ")\n",
    "\n",
    "# Build a PyTorch dataloader.\n",
    "dataloader = torch.utils.data.DataLoader(\n",
    "    dataset,                # Pass the dataset to the dataloader.\n",
    "    batch_size=128,         # A large batch size helps with learning.\n",
    "    shuffle=True,           # Shuffling is important!\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deploy model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "from lightly.loss import NTXentLoss # many losses are supported\n",
    "from lightly.models.modules.heads import SimCLRProjectionHead # Use MLP to map input features to a lower dimensional space\n",
    "import timm\n",
    "\n",
    "# torchvision to load resnet backbone\n",
    "#resnet = torchvision.models.resnet18()\n",
    "#resnet = torch.nn.Sequential(*list(resnet.children())[:-1]) # remove last classifier layer\n",
    "\n",
    "# timm lib to load resnet backbone\n",
    "resnet = timm.create_model('resnet18', pretrained=True)\n",
    "resnet.reset_classifier(0) # remove last classifier layer\n",
    "\n",
    "# build a SimCLR model\n",
    "class SimCLR(torch.nn.Module):\n",
    "    def __init__(self, backbone, hidden_dim, out_dim):\n",
    "        super().__init__()\n",
    "        self.backbone = backbone\n",
    "        self.projection_head = SimCLRProjectionHead(hidden_dim, hidden_dim, out_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = self.backbone(x).flatten(start_dim=1)\n",
    "        z = self.projection_head(h)\n",
    "        return z\n",
    "\n",
    "model = SimCLR(resnet, hidden_dim=512, out_dim=128)\n",
    "criterion = NTXentLoss(temperature=0.5) # (normalized temperature-scaled cross entropy loss)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-0, weight_decay=1e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Launch training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:5.30142879486084\n",
      "Epoch 2/10:5.437026500701904\n",
      "Epoch 3/10:5.502232074737549\n",
      "Epoch 4/10:5.425705432891846\n",
      "Epoch 5/10:5.390667915344238\n",
      "Epoch 6/10:5.379733562469482\n",
      "Epoch 7/10:5.440232276916504\n",
      "Epoch 8/10:5.428023338317871\n",
      "Epoch 9/10:5.513458728790283\n",
      "Epoch 10/10:5.411945819854736\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "max_epochs = 10\n",
    "for epoch in range(max_epochs):\n",
    "    for (x0, x1), _, _ in dataloader:\n",
    "\n",
    "        x0 = x0.to(device)\n",
    "        x1 = x1.to(device)\n",
    "\n",
    "        z0 = model(x0)\n",
    "        z1 = model(x1)\n",
    "\n",
    "        loss = criterion(z0, z1)\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "    print(f'Epoch {epoch+1}/{max_epochs}:{loss}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Embedding save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1000, 512])\n"
     ]
    }
   ],
   "source": [
    "# make a new dataloader without the transformations\n",
    "# The only transformation needed is to make a torch tensor out of the PIL image\n",
    "dataset.transform = torchvision.transforms.ToTensor()\n",
    "dataloader = torch.utils.data.DataLoader(\n",
    "    dataset,        # use the same dataset as before\n",
    "    batch_size=1,   # we can use batch size 1 for inference\n",
    "    shuffle=False,  # don't shuffle your data during inference\n",
    ")\n",
    "\n",
    "# embed your image dataset\n",
    "# Use the trained model to extract features and characterize new data to support downstream tasks such as reasoning, retrieval, and clustering\n",
    "embeddings = [] \n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for img, label, fnames in dataloader:\n",
    "        img = img.to(model.device)\n",
    "        emb = model.backbone(img).flatten(start_dim=1)\n",
    "        embeddings.append(emb)\n",
    "\n",
    "    embeddings = torch.cat(embeddings, 0)\n",
    "print(embeddings.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pytorch lightning framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name            | Type                 | Params | Mode \n",
      "-----------------------------------------------------------------\n",
      "0 | backbone        | ResNet               | 11.2 M | train\n",
      "1 | projection_head | SimCLRProjectionHead | 328 K  | train\n",
      "2 | criterion       | NTXentLoss           | 0      | train\n",
      "-----------------------------------------------------------------\n",
      "11.5 M    Trainable params\n",
      "0         Non-trainable params\n",
      "11.5 M    Total params\n",
      "46.022    Total estimated model params size (MB)\n",
      "103       Modules in train mode\n",
      "0         Modules in eval mode\n",
      "c:\\Users\\zhang\\miniconda3\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\zhang\\miniconda3\\Lib\\site-packages\\pytorch_lightning\\loops\\fit_loop.py:298: The number of training batches (8) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bbda05a63d544d7f923e4907b4dabd06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    }
   ],
   "source": [
    "import pytorch_lightning as pl\n",
    "\n",
    "# class SimCLR(torch.nn.Module):\n",
    "#     def __init__(self, backbone, hidden_dim, out_dim):\n",
    "#         super().__init__()\n",
    "#         self.backbone = backbone\n",
    "#         self.projection_head = SimCLRProjectionHead(hidden_dim, hidden_dim, out_dim)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         h = self.backbone(x).flatten(start_dim=1)\n",
    "#         z = self.projection_head(h)\n",
    "#         return z\n",
    "        \n",
    "class SimCLR(pl.LightningModule):\n",
    "    def __init__(self, backbone, hidden_dim, out_dim):\n",
    "        super().__init__()\n",
    "        self.backbone = backbone\n",
    "        self.projection_head = SimCLRProjectionHead(hidden_dim, hidden_dim, out_dim)\n",
    "        self.criterion = NTXentLoss(temperature=0.5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = self.backbone(x).flatten(start_dim=1)\n",
    "        z = self.projection_head(h)\n",
    "        return z\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        (x0, x1), _, _ = batch\n",
    "        z0 = self.forward(x0)\n",
    "        z1 = self.forward(x1)\n",
    "        loss = self.criterion(z0, z1)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.SGD(self.parameters(), lr=1e-0)\n",
    "        return optimizer\n",
    "\n",
    "max_epochs = 10\n",
    "\n",
    "model = SimCLR(resnet, hidden_dim=512, out_dim=128)\n",
    "trainer = pl.Trainer(max_epochs=max_epochs, devices=1, accelerator=\"gpu\")\n",
    "trainer.fit(model, dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concepts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightly.transforms import SimCLRTransform\n",
    "# customize the transform\n",
    "transform = SimCLRTransform(\n",
    "    input_size=128,   # resize input images to 128x128 pixels\n",
    "    cj_prob=0.0,      # disable color jittering\n",
    "    rr_prob=0.5,      # apply random rotation by 90 degrees with 50% probability\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms as T\n",
    "from lightly.transforms.multi_view_transform import MultiViewTransform\n",
    "\n",
    "# Create a global view transform that crops 224x224 patches from the input image.\n",
    "global_view = T.Compose([\n",
    "    T.RandomResizedCrop(size=224, scale=(0.08, 1.0)),\n",
    "    T.RandomHorizontalFlip(p=0.5),\n",
    "    T.RandomGrayscale(p=0.5),\n",
    "    T.ToTensor(),\n",
    "])\n",
    "\n",
    "# Create a local view transform that crops a random portion of the input image and resizes it to a 96x96 patch.\n",
    "local_view = T.Compose([\n",
    "    T.RandomResizedCrop(size=96, scale=(0.05, 0.4)),\n",
    "    T.RandomHorizontalFlip(p=0.5),\n",
    "    T.RandomGrayscale(p=0.5),\n",
    "    T.ToTensor(),\n",
    "])\n",
    "\n",
    "# Combine the transforms. Every transform will create one view.\n",
    "# The final transform will create four views: two global and two local views.\n",
    "transform = MultiViewTransform([global_view, global_view, local_view, local_view])\n",
    "views = transform(image) # return 4 transformed views"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.6 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d0af3871f74fa798a0a222812f3db7ca3ddb43c51379edde28baf2323a73ac20"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
